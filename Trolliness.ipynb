{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Russian Troll-Tweet Identifier\n",
    "\n",
    "This project uses a [Kaggle dataset](https://www.kaggle.com/vikasg/russian-troll-tweets) of 200,000 Tweets captured by NBC during the 2016 presidential election, which are known to be of malicious intent. However, as a 2016 political troll, the goal was to stir up as much polarization as possible while blending in as an average American citizen. \n",
    "<br><br>\n",
    "I attempt to train various sci-kit learn machine learning algorithims to classify a tweet as a troll (True) or a non-troll (False). In doing so, I add in certain features that touch on certain qualities of grammar patterns within non-native English speakers, as well as the intent behind a \"troll tweet\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Main Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/parkerglenn/Desktop/russian-troll-tweets/tweets.csv\")\n",
    "df = df[[\"user_key\",\"hashtags\",\"text\",\"mentions\"]]\n",
    "#Just because the data is so uneven right now\n",
    "df = df.drop(df.index[:200000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the non-troll set of tweets, a total of 3,476 tweets from 14 accounts were scraped using BeautifulSoup and the Twitter API. This was originally used for my LING 111 project, but below is the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    " \n",
    "# Consumer keys and access tokens, used for OAuth\n",
    "consumer_key = ''\n",
    "consumer_secret = ''\n",
    "access_token = ''\n",
    "access_token_secret = ''\n",
    " \n",
    "# OAuth process, using the keys and tokens\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    " \n",
    "# Creation of the actual interface, using authentication\n",
    "api = tweepy.API(auth)\n",
    " \n",
    "accounts = ['realDonaldTrump','SamuelLJackson', 'KimKardashian', 'BarackObama',\n",
    "                    'TheEllenShow', 'Oprah', 'jtimberlake', 'ladygaga', 'ArianaGrande',\n",
    "                    'justinbieber', 'OzzyOsbourne', 'SofiaVergara', 'shakira', 'britneyspears' ]\n",
    "\n",
    "# gets the last 200 tweets of a person, prints \n",
    "with open(\"TwitterCorpus.txt\", \"w\") as f:\n",
    "    for person in accounts:\n",
    "        tweet= api.user_timeline(screen_name=person,count=200,tweet_mode=\"extended\") \n",
    "        for status in tweet:\n",
    "            f.write(status._json['full_text'] + '\\n') # writes the text in file "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turning the json into a dictionary format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changes a newline separated text into a python dictionary format\n",
    "with open('TwitterCorpus.txt', 'r') as f: \n",
    "    with open('TwitterCorpusFormatted.py', 'w') as f2:\n",
    "        f2.write('TwitterCorpus = {')\n",
    "        counter = 1\n",
    "        for line in f.readlines():\n",
    "            f2.write('\\'Twitter_' + str(counter) + '\\':\\'')\n",
    "            counter += 1\n",
    "            f2.write(line.strip().replace('\\'', '\\\\\\'') + '\\',') # escape quote marks so they won't be treated as close quotes\n",
    "        f2.write('}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/Users/parkerglenn/Desktop/UCSB/ComputationalLinguistics/FinalProject\")\n",
    "from TwitterCorpusFormatted import TwitterCorpus\n",
    "\n",
    "tweets = []\n",
    "for i in TwitterCorpus.values():\n",
    "    tweets.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3482 tweets in the troll set, and 3476 in the non-troll set\n"
     ]
    }
   ],
   "source": [
    "print(\"There are {} tweets in the troll set, and {} in the non-troll set\".format(len(df),len(TwitterCorpus)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite starting out with the 200,000 Kaggle tweets, I use only 3,482 for efficiency and to match the non-troll set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "good = pd.DataFrame(tweet for tweet in TwitterCorpus.values())\n",
    "\n",
    "good.columns = [\"text\"]\n",
    "good[\"trolliness\"] = False\n",
    "df[\"trolliness\"] = True\n",
    "df = df.append(good, sort = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_key</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>text</th>\n",
       "      <th>mentions</th>\n",
       "      <th>trolliness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200000</th>\n",
       "      <td>mr_clampin</td>\n",
       "      <td>[]</td>\n",
       "      <td>RT @GeffGefferson1: Thank You!!! https://t.co/...</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200001</th>\n",
       "      <td>johnbranchh</td>\n",
       "      <td>[]</td>\n",
       "      <td>RT @blicqer: Former Auburn star joins Buccanee...</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200002</th>\n",
       "      <td>brightandglory</td>\n",
       "      <td>[\"TrumpsFavoriteHeadline\"]</td>\n",
       "      <td>RT @DanaGeezus: #TrumpsFavoriteHeadline List O...</td>\n",
       "      <td>[\"danageezus\"]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200003</th>\n",
       "      <td>cookncooks</td>\n",
       "      <td>[\"Brussels\"]</td>\n",
       "      <td>#Brussels #IslamKills Syrian refugees are not ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200004</th>\n",
       "      <td>paulinett</td>\n",
       "      <td>[]</td>\n",
       "      <td>RT @blicqer: Nicki Minaj's Harasses Homeless W...</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              user_key                    hashtags  \\\n",
       "200000      mr_clampin                          []   \n",
       "200001     johnbranchh                          []   \n",
       "200002  brightandglory  [\"TrumpsFavoriteHeadline\"]   \n",
       "200003      cookncooks                [\"Brussels\"]   \n",
       "200004       paulinett                          []   \n",
       "\n",
       "                                                     text        mentions  \\\n",
       "200000  RT @GeffGefferson1: Thank You!!! https://t.co/...              []   \n",
       "200001  RT @blicqer: Former Auburn star joins Buccanee...              []   \n",
       "200002  RT @DanaGeezus: #TrumpsFavoriteHeadline List O...  [\"danageezus\"]   \n",
       "200003  #Brussels #IslamKills Syrian refugees are not ...              []   \n",
       "200004  RT @blicqer: Nicki Minaj's Harasses Homeless W...              []   \n",
       "\n",
       "        trolliness  \n",
       "200000        True  \n",
       "200001        True  \n",
       "200002        True  \n",
       "200003        True  \n",
       "200004        True  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I defined \"success\" in this project, I include the Precision score on top of the F1 score because of its importance in this task. Since precision puts great weight false positives, it offers a gauge of potential repurcussions; in this instance a false positive would be a non-troll account being flagged as a troll account, a situation that would get Twitter in very hot water."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1064,
   "metadata": {},
   "outputs": [],
   "source": [
    "def success(*kwargs):\n",
    "    for column in kwargs:\n",
    "        print(\"Scores for model\", column)\n",
    "        print(\"Classification accuracy:\", sklearn.metrics.accuracy_score(output.Actual, output[column]))\n",
    "        print(\"F1 score:\", sklearn.metrics.f1_score(output.Actual, output[column], average = \"micro\"))\n",
    "        print(\"Precision:\", sklearn.metrics.precision_score(output.Actual, output[column], average = \"micro\"))\n",
    "        print(\"Matthews coefficient:\", sklearn.metrics.matthews_corrcoef(output.Actual, output[column]))\n",
    "        print()\n",
    "def success2(*kwargs):\n",
    "    for column in kwargs:\n",
    "        print(\"Scores for model\", column)\n",
    "        print(\"Classification accuracy:\", sklearn.metrics.accuracy_score(output2.Actual, output2[column]))\n",
    "        print(\"F1 score:\", sklearn.metrics.f1_score(output2.Actual, output2[column], average = \"micro\"))\n",
    "        print(\"Precision:\", sklearn.metrics.precision_score(output2.Actual, output2[column], average = \"micro\"))\n",
    "        print(\"Matthews coefficient:\", sklearn.metrics.matthews_corrcoef(output2.Actual, output2[column]))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1065,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import sklearn.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic MNB Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1066,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = sklearn.model_selection.train_test_split(df)\n",
    "output = pd.DataFrame(\n",
    "    {\n",
    "        \"Actual\": test.trolliness\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1070,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores for model default_MNB\n",
      "Classification accuracy: 0.8781609195402299\n",
      "F1 score: 0.8781609195402299\n",
      "Precision: 0.8781609195402299\n",
      "Matthews coefficient: 0.7567111872549714\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(stop_words = \"english\")\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "default_pipeline = sklearn.pipeline.make_pipeline(\n",
    "    vectorizer,\n",
    "    mnb,\n",
    ")\n",
    "\n",
    "default_pipeline.fit(train.text,train.trolliness)    \n",
    "output[\"default_MNB\"] = default_pipeline.predict(test.text)\n",
    "\n",
    "success(\"default_MNB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNB Model with Entity Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below this I modify a named entity recognition that I've used in the past to fit this data. It tokenizes a choses subset of Spacy's ents as a single token, if it was previously a multi-token term. For example, \"United Nations\" would be tokenized as one whole token, rather than the typical \"united\",\"nations\" distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For my ner tokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from spacy.gold import iob_to_biluo\n",
    "nlp = spacy.load('en_core_web_md', disable=['parser','tagger','textcat'])\n",
    "nlp.vocab.add_flag(lambda s: s.lower() in spacy.lang.en.stop_words.STOP_WORDS, spacy.attrs.IS_STOP)\n",
    "english_stopwords = stopwords.words('english')\n",
    "\n",
    "def ent_tokenizer(text):    \n",
    "    good_ents = [\"PERSON\",\"GPE\",\"ORG\", \"LOC\", \"EVENT\", \"FAC\", ]\n",
    "    continue_tags = [\"B-\",\"I-\"]\n",
    "    end_tags = [\"L-\",\"U-\"]\n",
    "    \n",
    "    toks = []\n",
    "    iobs = [i.ent_iob_ for i in nlp(text)]\n",
    "    biluos = list(iob_to_biluo(iobs))\n",
    "    index = -1\n",
    "    #Named entities variable\n",
    "    ne = \"\"\n",
    "    \n",
    "    for tok in nlp(text):\n",
    "        index += 1\n",
    "        if biluos[index] in continue_tags and str(tok.ent_type_) in good_ents:\n",
    "            #str(tok).split() != [] Checks if empty token\n",
    "            #For some reason tok.whitespace_ doesn't include double token entities\n",
    "            #like \"JENNIFER LAWRENCE\"\n",
    "            if str(tok).split() != []:\n",
    "                ne += \" \" + str(tok)\n",
    "        elif biluos[index] in end_tags and str(tok.ent_type_) in good_ents:\n",
    "            if str(tok).split() != []:\n",
    "                ne += \" \" + str(tok)\n",
    "                toks.append(ne.lstrip())\n",
    "                ne = \" \"\n",
    "        #If token is just a boring old word\n",
    "        else:\n",
    "            if tok.is_punct == False and tok.is_space == False and str(tok).lower() not in english_stopwords and not (str(tok)[0] == '@' or str(tok)[0] == '#' or str(tok).startswith('http') or str(tok).startswith('www')):\n",
    "                toks.append(str(tok).lower())\n",
    "    return toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['funny',\n",
       "  'see',\n",
       "  'little',\n",
       "  'Adam Schitt',\n",
       "  'ca',\n",
       "  'talking',\n",
       "  'fact',\n",
       "  'acting',\n",
       "  'attorney',\n",
       "  'general',\n",
       "  'Matt Whitaker',\n",
       "  'approved',\n",
       "  'Senate',\n",
       "  'mentioning',\n",
       "  'fact',\n",
       "  'Bob Mueller',\n",
       "  'highly',\n",
       "  'conflicted',\n",
       "  'approved',\n",
       "  'Senate']]"
      ]
     },
     "execution_count": 1178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = [ent_tokenizer(df.text.loc[123])]\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how, in the above example, names (\"Bob Mueller\",\"Matt Whitaker\") are successfully identified as an entity, and appropriately merged into a single token.\n",
    "\n",
    "### NOTE: the below cell of code takes a while to run, since it's relying on Spacy's NER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores for model default_MNB\n",
      "Classification accuracy: 0.8781609195402299\n",
      "F1 score: 0.8781609195402299\n",
      "Precision: 0.8781609195402299\n",
      "Matthews coefficient: 0.7567111872549714\n",
      "\n",
      "Scores for model ent_MNB\n",
      "Classification accuracy: 0.8844827586206897\n",
      "F1 score: 0.8844827586206897\n",
      "Precision: 0.8844827586206897\n",
      "Matthews coefficient: 0.7692739700805095\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ent_vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(tokenizer = ent_tokenizer)\n",
    "mnb = MultinomialNB()\n",
    "ent_pipeline = sklearn.pipeline.make_pipeline(\n",
    "    ent_vectorizer,\n",
    "    mnb,\n",
    ")\n",
    "\n",
    "ent_pipeline.fit(train.text,train.trolliness)    \n",
    "output[\"ent_MNB\"] = ent_pipeline.predict(test.text)\n",
    "\n",
    "success(\"default_MNB\", \"ent_MNB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So adding in entity tokenization helped a bit, but not a significant amount. Because the result wasn't some crazy improvement, in the future examples I use a more traditional tokenizer to avoid the processing times that come with Spacy's NER."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Grammar Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further assist the sci-kit models in identifying troll tweets, I began to look for grammatical clues in the framework of the tweets. Essentially, the two features described below center around the idea that non-native English speakers, specifically native Russian speakers, often struggle with correctly utilizing English articles (a, an, the), along with other stop words. \n",
    "<br/><br/>\n",
    "The dataset used to calibrate the grammar features is from LING 110. It's a set of transcriptions from a senate hearing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1072,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "files = glob.glob(\"/Users/parkerglenn/Desktop/russian-troll-tweets/CapitolWords/*.txt\")\n",
    "capitol = []\n",
    "for file in files:\n",
    "    new = open(file)\n",
    "    capitol.append(new.read())\n",
    "    new.close()\n",
    "\n",
    "# Just becuase there's so many. My computer can't handle everything. \n",
    "capitol = capitol[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I yield myself an additional 30 seconds.\n",
      "It is restrictive in some respects; in some it is not. We do not require in the first 5 years -- in the original bill you have to do these reviews and have to provide some service, so that is not the same. The Breaux amendment in fact goes further. In the second 5 years there was an allowance in the conference report, I believe, and I can check on that, that after 5 years they could use Federal funds.\n"
     ]
    }
   ],
   "source": [
    "print(capitol[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_tweets(text):\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        # Takes out tags, hashtags, and urls\n",
    "        if not (token[0] == '@' or token[0] == '#' or token.startswith('http') or token.startswith('www')):\n",
    "            # Filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "            if re.search('[a-zA-Z]', token):\n",
    "                filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating \"stop_score\"\n",
    "Studies have shown that non-native English speakers tend to use stop words at a less frequent rate than native English speakers. Intuitively, this makes sense; they're tiny grammatical particles that, to the casual learner of English, may not seem as important as the \"big\" vocab words. \n",
    "<br/><br/>\n",
    "To calculate \"stop_score\" as a feature, the average stopword usage per a sentence of x-length is calculated across the senate hearing transcripts. Then, the amount of stopwords in a tweet of X-length is calculated, and:\n",
    "<br/><br/>**stop_score = (stopwords in a tweet of xlength) - (avg. stopwords in a sentence of xlength)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1193,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "english_stopwords = stopwords.words('english')\n",
    "tokenizer = nltk.tokenize.TweetTokenizer()\n",
    "\n",
    "sent_dict = {}\n",
    "for text in capitol:\n",
    "    sents = [sent for sent in nltk.sent_tokenize(text)]\n",
    "    for sent in sents:\n",
    "        toks = [word for word in tokenizer.tokenize(sent)]\n",
    "        try:\n",
    "            sent_dict[len(toks)]\n",
    "        except KeyError:\n",
    "            sent_dict[len(toks)] = []\n",
    "        stops = 0\n",
    "        for tok in toks:\n",
    "            if tok in english_stopwords:\n",
    "                stops += 1\n",
    "        sent_dict[len(toks)].append(stops)\n",
    "   \n",
    "# Calculates average amount of stopwords for a sentence of x length     \n",
    "for k in sent_dict:\n",
    "    sent_dict[k] = sum(sent_dict[k]) / len(sent_dict[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1194,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bad_words = [\"RT\",\"@\",\"https\",\"//\"]\n",
    "stop_scores = []\n",
    "for place, tweet in enumerate(df.text):\n",
    "    score = 0\n",
    "    sents = [sent for sent in nltk.sent_tokenize(tweet)]\n",
    "    for sent in sents:\n",
    "        toks = [word for word in tokenizer.tokenize(sent) if word not in bad_words and word.isalpha()]\n",
    "        count = 0\n",
    "        for tok in toks:\n",
    "            if tok in english_stopwords:\n",
    "                count += 1\n",
    "        try:\n",
    "            score += count - sent_dict[len(toks)]\n",
    "        except:\n",
    "            pass\n",
    "    stop_scores.append(score)\n",
    "df[\"stop_score\"] = stop_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I classify the stop_scores into 3 groups called \"suspicion,\" as those scores with lower values are more suspicious. I end up not using this as a feature though, and just go with the raw \"stop_score\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1076,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stops_group(x):\n",
    "    if x > -2:\n",
    "        return(0)\n",
    "    if x < -1.1 and x > -4:\n",
    "        return(1)\n",
    "    if x < -4:\n",
    "        return(2)\n",
    "df[\"suspicion\"] = [stops_group(x) for x in df.stop_score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>stop_score</th>\n",
       "      <th>trolliness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200000</th>\n",
       "      <td>RT @GeffGefferson1: Thank You!!! https://t.co/I4ravmchMy</td>\n",
       "      <td>-0.001168</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200001</th>\n",
       "      <td>RT @blicqer: Former Auburn star joins Buccaneers after promising to help his homeless mom https://t.co/QRnvFfJufu @NewPghCourier https://t.‚Ä¶</td>\n",
       "      <td>-1.495528</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200002</th>\n",
       "      <td>RT @DanaGeezus: #TrumpsFavoriteHeadline List Of Best People Released. Trump no1</td>\n",
       "      <td>-1.394577</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200003</th>\n",
       "      <td>#Brussels #IslamKills Syrian refugees are not welcome in my neighborhood! You garbage Liberal!</td>\n",
       "      <td>0.841558</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200004</th>\n",
       "      <td>RT @blicqer: Nicki Minaj's Harasses Homeless Woman With Mental Illness https://t.co/d7S17rgbry @Thought_Crimez https://t.co/WcIBAOFnP6</td>\n",
       "      <td>-2.412750</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200005</th>\n",
       "      <td>RT @Fusion: Donald Trump was apparently so surprised to see a black person at his rally, he didn't recognize him as a supporter‚Äîand called‚Ä¶</td>\n",
       "      <td>2.303869</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200006</th>\n",
       "      <td>Italienisches Parlament stimmt f√ºr Milit√§reinsatz vor Libyen. Aber die #Fl√ºchtlinge werden neue Wege finden!\\r\\nhttps://t.co/h5jDICWktg</td>\n",
       "      <td>-4.325325</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200007</th>\n",
       "      <td>RT @Counselor70: @eff_dblu_ell @colavs2184 CNN &amp;amp; MSM TELL TRUTH OR GET THE FUCK OUT, THE\\r\\nSTENCH OF LIARS Those Bias Bastards https://t.co/d‚Ä¶</td>\n",
       "      <td>-6.156216</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200008</th>\n",
       "      <td>RT @fedbizop: #ItsRiskyTo  get this hash tag confused with the Run DMC song It's Tricky.</td>\n",
       "      <td>-1.067153</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200009</th>\n",
       "      <td>RT @RT_America: WaPo begs people to stop saying 'fake news' after they started it all [VIDEO] https://t.co/Dseyo075fM @TheResident https://‚Ä¶</td>\n",
       "      <td>-0.231892</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200010</th>\n",
       "      <td>Harambe is dead</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200011</th>\n",
       "      <td>Jeweler says Trump and Clinton are scaring couples  #politics</td>\n",
       "      <td>-0.885714</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200012</th>\n",
       "      <td>#ThingsMoreTrustedThanHillary Congress to control spending</td>\n",
       "      <td>0.175103</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200013</th>\n",
       "      <td>Today #ChildrenThinkThat socialism is good. Thanks, Obama! https://t.co/BFP1Ugqlaj</td>\n",
       "      <td>0.173934</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200014</th>\n",
       "      <td>RT @JustMyIntuition: And don't look back. üíØ https://t.co/NusKthov7J</td>\n",
       "      <td>-0.272727</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200015</th>\n",
       "      <td>RT @Toupsfamily: Do you trust the mainstream media? \\r\\n\\r\\n#MAGA\\r\\n#TruthMatters \\r\\n\\r\\n#RETWEET</td>\n",
       "      <td>0.087425</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200016</th>\n",
       "      <td>RT @tjemery1: Donald Trump often made donations to state attorneys general reviewing his businesses https://t.co/ymViQqY4h1 #NeverTrump</td>\n",
       "      <td>-2.495528</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200017</th>\n",
       "      <td>RT @SueinRockville: @ezlusztig @dpakman\\r\\nKeep talking #45\\r\\nOne day closer to the Impeachment, Indictment of an Illegitimately Installed Imbec‚Ä¶</td>\n",
       "      <td>-1.231892</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200018</th>\n",
       "      <td>You can call me whoever you want but I don't think it's nornal to see this in the US https://t.co/E7L8OzZzJy</td>\n",
       "      <td>1.509096</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200019</th>\n",
       "      <td>#NationalGirfriendDay</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200020</th>\n",
       "      <td>RT @AllenWestRepub: LOL! WHOOPIE GOLDBERG And JOY BEHAR Say Trump Needs To \"Step Down\" Before Inauguration Over \"Russian Hacker\"... https:/‚Ä¶</td>\n",
       "      <td>-6.156216</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200021</th>\n",
       "      <td>#HowToConfuseAMillennial explain them that facts are not racist..\\n#BLM is just a hate group.. https://t.co/KlElrKGMdv</td>\n",
       "      <td>2.504472</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200022</th>\n",
       "      <td>RT @tsbarnes89: ‚ÄòYou are fascinated with sex‚Äô: That Megyn Kelly-Newt Gingrich showdown was one for the ages https://t.co/7aRNLLARc7</td>\n",
       "      <td>-0.231892</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200023</th>\n",
       "      <td>HOLDING. #POLICE. ACCOUNTABLE.</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200024</th>\n",
       "      <td>RT @JohnKStahlUSA: Share this with younger people in your life. We don't need another national embarrassment. #tcot #ccot #gop #maga https:‚Ä¶</td>\n",
       "      <td>-0.280292</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200025</th>\n",
       "      <td>Harold and Kumar Make a Porno #AddAMovieRuinAMovie</td>\n",
       "      <td>0.087425</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200026</th>\n",
       "      <td>#HowToLoseYourJob Go work for a newspaper</td>\n",
       "      <td>0.605423</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200027</th>\n",
       "      <td>RT @deray: .@SpeakerRyan, why doesn't your party want to know the truth? https://t.co/UU4Qlf8Ifl</td>\n",
       "      <td>1.114286</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200028</th>\n",
       "      <td>RT @Skye_McNeil7: I lost a cockroach somewhere between the plates #RuinADinnerInOnePhrase</td>\n",
       "      <td>0.114286</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200029</th>\n",
       "      <td>RT @Conservatexian: News post: \"Trump¬ís F.C.C. Pick Quickly Targets Net Neutrality Rules\" https://t.co/y5opzfnI4A</td>\n",
       "      <td>-3.325325</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3446</th>\n",
       "      <td>üëíüëíüëí #fbf</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3447</th>\n",
       "      <td>üì∑: Gilles Bensimon https://t.co/2XvfKnnD0c</td>\n",
       "      <td>-0.001168</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3448</th>\n",
       "      <td>I‚Äôm so grateful for the guidance my mother has given me throughout my life! My intuition, will to be stronger and love for making each day count comes from her!!! Hope you have an amazing b-day!!!...</td>\n",
       "      <td>6.784441</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3449</th>\n",
       "      <td>Stronger together üëØ‚Äç‚ôÄÔ∏èüçèüëØ‚Äç‚ôÄÔ∏è https://t.co/wtXt10SclD</td>\n",
       "      <td>-0.001168</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3450</th>\n",
       "      <td>üíÉüèºüíÉüèºüíÉüèº https://t.co/SR82xbZVi8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3451</th>\n",
       "      <td>#FBF üë†üë†üë† photo by Steven Klein https://t.co/sGz207mCEL</td>\n",
       "      <td>0.175103</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3452</th>\n",
       "      <td>üé© #tbt</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3453</th>\n",
       "      <td>üì∑: @EllenVUnwerth https://t.co/UPsSl2wv42</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3454</th>\n",
       "      <td>RT @COTA: Gimme more. Tickets on sale Friday at https://t.co/JafXlc8WRR https://t.co/QjfBBgldUN</td>\n",
       "      <td>1.604254</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3455</th>\n",
       "      <td>love getting to watch my boys at the skate park!! https://t.co/7m1mVZG3qY</td>\n",
       "      <td>0.311590</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3456</th>\n",
       "      <td>Switching up my workout routine in the gym as I get ready for the #PieceOfMe Tour!! üéÄ https://t.co/dqZJkHzsRs</td>\n",
       "      <td>1.336403</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3457</th>\n",
       "      <td>Sending my love to the people of Toronto and all those affected ‚ù§Ô∏è https://t.co/sJW1D7Mr3F</td>\n",
       "      <td>2.504472</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3458</th>\n",
       "      <td>This video shoot was incredible!!! What dreams are made of... #Slave4U https://t.co/XIvSzazUwH</td>\n",
       "      <td>0.210845</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3459</th>\n",
       "      <td>Happy birthday to my big brother Bryan!!! He means the world to me and I hope he has the best b-day ever!!!! https://t.co/5SRfYHJckO</td>\n",
       "      <td>1.355358</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3460</th>\n",
       "      <td>I think it's impossible for me to go a day without dancing üòúüåπ https://t.co/TQnNSZWdIU</td>\n",
       "      <td>-0.067153</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3461</th>\n",
       "      <td>RT @TheBritneyArmy: Don't miss @britneyspears on the @GLAAD Awards airing tonight on @logotv at 8/7c!! #GLAADawards https://t.co/9FnnSBQG0s</td>\n",
       "      <td>1.114286</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3462</th>\n",
       "      <td>üçé‚úèÔ∏èüçé‚úèÔ∏èüçé https://t.co/gBVJhtw7oO</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3463</th>\n",
       "      <td>üå∏ https://t.co/DxjtIUjMpW</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3464</th>\n",
       "      <td>Channeling my inner Aretha Franklin - the woman has got so much soul!!!! ‚≠êÔ∏èüçé‚≠êÔ∏è https://t.co/peq2KiOV0M</td>\n",
       "      <td>-0.495528</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3465</th>\n",
       "      <td>https://t.co/YaaKwCz6pT</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3466</th>\n",
       "      <td>üçéüçéüçé https://t.co/vHKsU8JVDl</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3467</th>\n",
       "      <td>some days you just got to dance! https://t.co/Lhc3s9C894</td>\n",
       "      <td>1.587250</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3468</th>\n",
       "      <td>What an incredibly moving evening celebrating love and acceptance at the #GLAADawards! I'm so proud to be an ally of the LGBTQ community and it was an absolute honor to receive the @GLAAD Vanguard...</td>\n",
       "      <td>2.936361</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3469</th>\n",
       "      <td>‚ú®@glaad awards tonight ‚ú® #GLAADawards https://t.co/dO5CBc1Txx</td>\n",
       "      <td>-0.001168</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3470</th>\n",
       "      <td>I could not be more excited to welcome baby Ivey to this world. Congrats to @jamielynnspears and the whole family - I love you all so much!!! https://t.co/o3M8IdSS4h</td>\n",
       "      <td>2.638156</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3471</th>\n",
       "      <td>RT @glaad: Enter to win a chance to go to the #GLAADawards and see @britneyspears, @iamwandasykes, Jim Parsons and MORE! https://t.co/AufjU‚Ä¶</td>\n",
       "      <td>1.336403</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3472</th>\n",
       "      <td>Wishing my sister @jamielynnspears a very happy 27th birthday today!! üéÄüíï</td>\n",
       "      <td>0.114286</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3473</th>\n",
       "      <td>Heaven gained a beautiful angel this week with the passing of Christopher Metsker. His strength and fighting spirit was not only an inspiration to me, but the #BritneyArmy as well. My heart goes o...</td>\n",
       "      <td>4.564718</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3474</th>\n",
       "      <td>üçéüçéüçé https://t.co/73RuOK6HXc</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3475</th>\n",
       "      <td>Just want to wish happy birthday to my friend @iamstevent!!! You're a true rock star and inspiration!! Love you! üíï #HappyBDayStevenTyler</td>\n",
       "      <td>0.843779</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6958 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                           text  \\\n",
       "200000                                                                                                                                                 RT @GeffGefferson1: Thank You!!! https://t.co/I4ravmchMy   \n",
       "200001                                                             RT @blicqer: Former Auburn star joins Buccaneers after promising to help his homeless mom https://t.co/QRnvFfJufu @NewPghCourier https://t.‚Ä¶   \n",
       "200002                                                                                                                          RT @DanaGeezus: #TrumpsFavoriteHeadline List Of Best People Released. Trump no1   \n",
       "200003                                                                                                           #Brussels #IslamKills Syrian refugees are not welcome in my neighborhood! You garbage Liberal!   \n",
       "200004                                                                   RT @blicqer: Nicki Minaj's Harasses Homeless Woman With Mental Illness https://t.co/d7S17rgbry @Thought_Crimez https://t.co/WcIBAOFnP6   \n",
       "200005                                                              RT @Fusion: Donald Trump was apparently so surprised to see a black person at his rally, he didn't recognize him as a supporter‚Äîand called‚Ä¶   \n",
       "200006                                                                  Italienisches Parlament stimmt f√ºr Milit√§reinsatz vor Libyen. Aber die #Fl√ºchtlinge werden neue Wege finden!\\r\\nhttps://t.co/h5jDICWktg   \n",
       "200007                                                      RT @Counselor70: @eff_dblu_ell @colavs2184 CNN &amp; MSM TELL TRUTH OR GET THE FUCK OUT, THE\\r\\nSTENCH OF LIARS Those Bias Bastards https://t.co/d‚Ä¶   \n",
       "200008                                                                                                                 RT @fedbizop: #ItsRiskyTo  get this hash tag confused with the Run DMC song It's Tricky.   \n",
       "200009                                                             RT @RT_America: WaPo begs people to stop saying 'fake news' after they started it all [VIDEO] https://t.co/Dseyo075fM @TheResident https://‚Ä¶   \n",
       "200010                                                                                                                                                                                          Harambe is dead   \n",
       "200011                                                                                                                                            Jeweler says Trump and Clinton are scaring couples  #politics   \n",
       "200012                                                                                                                                               #ThingsMoreTrustedThanHillary Congress to control spending   \n",
       "200013                                                                                                                       Today #ChildrenThinkThat socialism is good. Thanks, Obama! https://t.co/BFP1Ugqlaj   \n",
       "200014                                                                                                                                      RT @JustMyIntuition: And don't look back. üíØ https://t.co/NusKthov7J   \n",
       "200015                                                                                                      RT @Toupsfamily: Do you trust the mainstream media? \\r\\n\\r\\n#MAGA\\r\\n#TruthMatters \\r\\n\\r\\n#RETWEET   \n",
       "200016                                                                  RT @tjemery1: Donald Trump often made donations to state attorneys general reviewing his businesses https://t.co/ymViQqY4h1 #NeverTrump   \n",
       "200017                                                       RT @SueinRockville: @ezlusztig @dpakman\\r\\nKeep talking #45\\r\\nOne day closer to the Impeachment, Indictment of an Illegitimately Installed Imbec‚Ä¶   \n",
       "200018                                                                                             You can call me whoever you want but I don't think it's nornal to see this in the US https://t.co/E7L8OzZzJy   \n",
       "200019                                                                                                                                                                                    #NationalGirfriendDay   \n",
       "200020                                                             RT @AllenWestRepub: LOL! WHOOPIE GOLDBERG And JOY BEHAR Say Trump Needs To \"Step Down\" Before Inauguration Over \"Russian Hacker\"... https:/‚Ä¶   \n",
       "200021                                                                                   #HowToConfuseAMillennial explain them that facts are not racist..\\n#BLM is just a hate group.. https://t.co/KlElrKGMdv   \n",
       "200022                                                                      RT @tsbarnes89: ‚ÄòYou are fascinated with sex‚Äô: That Megyn Kelly-Newt Gingrich showdown was one for the ages https://t.co/7aRNLLARc7   \n",
       "200023                                                                                                                                                                           HOLDING. #POLICE. ACCOUNTABLE.   \n",
       "200024                                                             RT @JohnKStahlUSA: Share this with younger people in your life. We don't need another national embarrassment. #tcot #ccot #gop #maga https:‚Ä¶   \n",
       "200025                                                                                                                                                       Harold and Kumar Make a Porno #AddAMovieRuinAMovie   \n",
       "200026                                                                                                                                                                #HowToLoseYourJob Go work for a newspaper   \n",
       "200027                                                                                                         RT @deray: .@SpeakerRyan, why doesn't your party want to know the truth? https://t.co/UU4Qlf8Ifl   \n",
       "200028                                                                                                                RT @Skye_McNeil7: I lost a cockroach somewhere between the plates #RuinADinnerInOnePhrase   \n",
       "200029                                                                                        RT @Conservatexian: News post: \"Trump¬ís F.C.C. Pick Quickly Targets Net Neutrality Rules\" https://t.co/y5opzfnI4A   \n",
       "...                                                                                                                                                                                                         ...   \n",
       "3446                                                                                                                                                                                                   üëíüëíüëí #fbf   \n",
       "3447                                                                                                                                                                 üì∑: Gilles Bensimon https://t.co/2XvfKnnD0c   \n",
       "3448    I‚Äôm so grateful for the guidance my mother has given me throughout my life! My intuition, will to be stronger and love for making each day count comes from her!!! Hope you have an amazing b-day!!!...   \n",
       "3449                                                                                                                                                        Stronger together üëØ‚Äç‚ôÄÔ∏èüçèüëØ‚Äç‚ôÄÔ∏è https://t.co/wtXt10SclD   \n",
       "3450                                                                                                                                                                             üíÉüèºüíÉüèºüíÉüèº https://t.co/SR82xbZVi8   \n",
       "3451                                                                                                                                                     #FBF üë†üë†üë† photo by Steven Klein https://t.co/sGz207mCEL   \n",
       "3452                                                                                                                                                                                                     üé© #tbt   \n",
       "3453                                                                                                                                                                  üì∑: @EllenVUnwerth https://t.co/UPsSl2wv42   \n",
       "3454                                                                                                            RT @COTA: Gimme more. Tickets on sale Friday at https://t.co/JafXlc8WRR https://t.co/QjfBBgldUN   \n",
       "3455                                                                                                                                  love getting to watch my boys at the skate park!! https://t.co/7m1mVZG3qY   \n",
       "3456                                                                                              Switching up my workout routine in the gym as I get ready for the #PieceOfMe Tour!! üéÄ https://t.co/dqZJkHzsRs   \n",
       "3457                                                                                                                 Sending my love to the people of Toronto and all those affected ‚ù§Ô∏è https://t.co/sJW1D7Mr3F   \n",
       "3458                                                                                                             This video shoot was incredible!!! What dreams are made of... #Slave4U https://t.co/XIvSzazUwH   \n",
       "3459                                                                       Happy birthday to my big brother Bryan!!! He means the world to me and I hope he has the best b-day ever!!!! https://t.co/5SRfYHJckO   \n",
       "3460                                                                                                                      I think it's impossible for me to go a day without dancing üòúüåπ https://t.co/TQnNSZWdIU   \n",
       "3461                                                                RT @TheBritneyArmy: Don't miss @britneyspears on the @GLAAD Awards airing tonight on @logotv at 8/7c!! #GLAADawards https://t.co/9FnnSBQG0s   \n",
       "3462                                                                                                                                                                            üçé‚úèÔ∏èüçé‚úèÔ∏èüçé https://t.co/gBVJhtw7oO   \n",
       "3463                                                                                                                                                                                  üå∏ https://t.co/DxjtIUjMpW   \n",
       "3464                                                                                                     Channeling my inner Aretha Franklin - the woman has got so much soul!!!! ‚≠êÔ∏èüçé‚≠êÔ∏è https://t.co/peq2KiOV0M   \n",
       "3465                                                                                                                                                                                    https://t.co/YaaKwCz6pT   \n",
       "3466                                                                                                                                                                                üçéüçéüçé https://t.co/vHKsU8JVDl   \n",
       "3467                                                                                                                                                   some days you just got to dance! https://t.co/Lhc3s9C894   \n",
       "3468    What an incredibly moving evening celebrating love and acceptance at the #GLAADawards! I'm so proud to be an ally of the LGBTQ community and it was an absolute honor to receive the @GLAAD Vanguard...   \n",
       "3469                                                                                                                                              ‚ú®@glaad awards tonight ‚ú® #GLAADawards https://t.co/dO5CBc1Txx   \n",
       "3470                                      I could not be more excited to welcome baby Ivey to this world. Congrats to @jamielynnspears and the whole family - I love you all so much!!! https://t.co/o3M8IdSS4h   \n",
       "3471                                                               RT @glaad: Enter to win a chance to go to the #GLAADawards and see @britneyspears, @iamwandasykes, Jim Parsons and MORE! https://t.co/AufjU‚Ä¶   \n",
       "3472                                                                                                                                   Wishing my sister @jamielynnspears a very happy 27th birthday today!! üéÄüíï   \n",
       "3473    Heaven gained a beautiful angel this week with the passing of Christopher Metsker. His strength and fighting spirit was not only an inspiration to me, but the #BritneyArmy as well. My heart goes o...   \n",
       "3474                                                                                                                                                                                üçéüçéüçé https://t.co/73RuOK6HXc   \n",
       "3475                                                                   Just want to wish happy birthday to my friend @iamstevent!!! You're a true rock star and inspiration!! Love you! üíï #HappyBDayStevenTyler   \n",
       "\n",
       "        stop_score  trolliness  \n",
       "200000   -0.001168        True  \n",
       "200001   -1.495528        True  \n",
       "200002   -1.394577        True  \n",
       "200003    0.841558        True  \n",
       "200004   -2.412750        True  \n",
       "200005    2.303869        True  \n",
       "200006   -4.325325        True  \n",
       "200007   -6.156216        True  \n",
       "200008   -1.067153        True  \n",
       "200009   -0.231892        True  \n",
       "200010    0.727273        True  \n",
       "200011   -0.885714        True  \n",
       "200012    0.175103        True  \n",
       "200013    0.173934        True  \n",
       "200014   -0.272727        True  \n",
       "200015    0.087425        True  \n",
       "200016   -2.495528        True  \n",
       "200017   -1.231892        True  \n",
       "200018    1.509096        True  \n",
       "200019    0.000000        True  \n",
       "200020   -6.156216        True  \n",
       "200021    2.504472        True  \n",
       "200022   -0.231892        True  \n",
       "200023    0.000000        True  \n",
       "200024   -0.280292        True  \n",
       "200025    0.087425        True  \n",
       "200026    0.605423        True  \n",
       "200027    1.114286        True  \n",
       "200028    0.114286        True  \n",
       "200029   -3.325325        True  \n",
       "...            ...         ...  \n",
       "3446      0.000000       False  \n",
       "3447     -0.001168       False  \n",
       "3448      6.784441       False  \n",
       "3449     -0.001168       False  \n",
       "3450      0.000000       False  \n",
       "3451      0.175103       False  \n",
       "3452      0.000000       False  \n",
       "3453      0.000000       False  \n",
       "3454      1.604254       False  \n",
       "3455      0.311590       False  \n",
       "3456      1.336403       False  \n",
       "3457      2.504472       False  \n",
       "3458      0.210845       False  \n",
       "3459      1.355358       False  \n",
       "3460     -0.067153       False  \n",
       "3461      1.114286       False  \n",
       "3462      0.000000       False  \n",
       "3463      0.000000       False  \n",
       "3464     -0.495528       False  \n",
       "3465      0.000000       False  \n",
       "3466      0.000000       False  \n",
       "3467      1.587250       False  \n",
       "3468      2.936361       False  \n",
       "3469     -0.001168       False  \n",
       "3470      2.638156       False  \n",
       "3471      1.336403       False  \n",
       "3472      0.114286       False  \n",
       "3473      4.564718       False  \n",
       "3474      0.000000       False  \n",
       "3475      0.843779       False  \n",
       "\n",
       "[6958 rows x 3 columns]"
      ]
     },
     "execution_count": 1196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"text\",\"stop_score\",\"trolliness\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1077,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(*args, **kw)>"
      ]
     },
     "execution_count": 1077,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.bar(scores.keys(), scores.values())\n",
    "plt.title(\"Distribution of stop_scores\")\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The leftward-skewing in the bargraph above implies that most tweets use less-than average amounts of stopwords per sentence. This may be partly due to the typically terse, informal context of the sentences (tweets) as opposed to the more formal context of the calibration corpus (senate hearing). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating \"mistake\" Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process detailed below is based on the fact that in the Russian language, both indefinite and definite articles are not present. As a result, the absence of such articles in English result in awkward speech by non-native speakers (ex. \"I took *train* to Washington\"). This incorrect grammatical structure is tough to define based solely on parts of speech, since some nouns require an article preceding it, while others don't (ex. \"I drank *water* before my run\"). A markov-like process was used to calculate the percentage of instances a specific noun was preceded by an article, and this percentage is used to flag incorrect grammar. \n",
    "<br/><br/> **mistakes = integer value of perceived grammar \"errors\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1101,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = [\"a\",\"an\",\"the\",\"this\",\"that\",\"those\",\"these\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I admit, I included the demonstrative pronouns \"this, that, these\" and \"those\" in a list titles \"articles.\" But after analyzing the tweets and examining the results of using strictly articles vs. the revised list found above, this iteration seemed to provide more polarizing, helpful results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1182,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_toks = []\n",
    "# Part of speech list is created for future use\n",
    "toks_pos = []\n",
    "def tokenizer(text):\n",
    "    toks = []\n",
    "    pos = []\n",
    "    text = text.lower()\n",
    "    for tok in nlp(text):\n",
    "        if tok.is_punct == False and tok.is_space == False:\n",
    "            pos.append(str(tok.pos_))\n",
    "            toks.append(str(tok))\n",
    "    tweet_toks.append(toks)\n",
    "    toks_pos.append(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in capitol:\n",
    "    tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar_dict = []\n",
    "for transcript_place, transcript in enumerate(text_toks): \n",
    "    for word_place, word in enumerate(transcript):\n",
    "        if word in articles: \n",
    "            try:\n",
    "                # If the word following the article is a noun\n",
    "                if toks_pos[transcript_place][word_place + 1] == \"NOUN\":\n",
    "                    # Then add that noun to the grammar_dict\n",
    "                    grammar_dict.append(transcript[word_place + 1].lower())\n",
    "            except IndexError:\n",
    "                pass\n",
    "            try:\n",
    "                # Below catches instance of phrases like \"he is a great candidate\", where an adjective seperates noun and article.\n",
    "                if toks_pos[transcript_place][word_place + 1] == \"ADJ\" and toks_pos[transcript_place][word_place + 2] == \"NOUN\":\n",
    "                    grammar_dict.append(transcript[word_place + 2].lower())\n",
    "            except IndexError:\n",
    "                pass\n",
    "from collections import Counter\n",
    "grammar_dict = dict(Counter(grammar_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This counts all occurences of the nouns in grammar_dict, regardless of if they are preceded by an article.\n",
    "count_dict = []\n",
    "for transcript in text_toks:\n",
    "    for tok in transcript:\n",
    "        if tok.lower() in list(grammar_dict.keys()):\n",
    "            count_dict.append(tok.lower())\n",
    "count_dict = dict(Counter(count_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below invokes the idea of ratio-ing: Out of all the times that a specific noun is used, what percentage of times is it preceded by an article?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in grammar_dict:\n",
    "    try:\n",
    "        grammar_dict[word] = grammar_dict[word] / count_dict[word]\n",
    "    except:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "259097"
      ]
     },
     "execution_count": 635,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#f = open(\"/Users/parkerglenn/Desktop/russian-troll-tweets/grammar_dict_WITHPRONOUNS.py\",\"w\")\n",
    "#f.write(\"grammar_dict = \" + str(grammar_dict))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'levin': 0.24786324786324787,\n",
       " 'cosponsor': 0.6854838709677419,\n",
       " 'moment': 0.9125799573560768,\n",
       " 'debate': 0.6253443526170799,\n",
       " 'members': 0.1877282688093499,\n",
       " 'body': 0.8542976939203354,\n",
       " 'status': 0.3623693379790941,\n",
       " 'president': 0.35006817900086773,\n",
       " 'hue': 1.0,\n",
       ...}"
      ]
     },
     "execution_count": 1106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grammar_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 1109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-loading Spacy because I had previously disabled the \"tagger\" that I now need\n",
    "nlp = spacy.load('en_core_web_md', disable=['parser','ner','textcat'])\n",
    "nlp.vocab.add_flag(lambda s: s.lower() in spacy.lang.en.stop_words.STOP_WORDS, spacy.attrs.IS_STOP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After many iterations, 0.75 is the value that I ended up using for the \"threshold\" because of the contrast I got between troll mistakes and non-troll mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1110,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.tokenize.TweetTokenizer()\n",
    "# This threshold is the cutoff for what is considered a \"mistake\"\n",
    "threshold = .75\n",
    "grammar_errors = []\n",
    "bad_words = []\n",
    "for tweet_place, tweet in enumerate(df.text):\n",
    "    bad_words_tweet = []\n",
    "    errors = 0\n",
    "    toks = tokenizer.tokenize(tweet)  \n",
    "    for tok_place, tok in enumerate(toks):  \n",
    "        tok = tok.lower()\n",
    "        try:\n",
    "            if toks[tok_place-1].lower() not in articles and toks[tok_place-2].lower() not in articles and tok in list(grammar_dict.keys()) and grammar_dict[tok] >= threshold:\n",
    "                # Checks to be sure the word functions as a noun. Only takes a small sample of the tweet to save time  \n",
    "                sent = \"\"\n",
    "                for x in range(-2,2):\n",
    "                    sent += toks[tok_place + x].lower()\n",
    "                    sent += \" \"  \n",
    "                for word in nlp(sent):\n",
    "                    if str(word) == tok:\n",
    "                        pos = str(word.pos_)\n",
    "                # Only if the word functions as a noun is it counted as an error.\n",
    "                # Before this, the phrase \"cowboy boots\" was getting flagged despite it being\n",
    "                # grammatically correct (\"cowboy\" acts as an adjective here)\n",
    "                if pos == \"NOUN\":     \n",
    "                    errors += 1\n",
    "                    bad_words_tweet.append(tok)\n",
    "        except IndexError:\n",
    "            pass\n",
    "    grammar_errors.append(errors)\n",
    "    bad_words.append(bad_words_tweet)\n",
    "df[\"grammar_errors\"] = grammar_errors\n",
    "df[\"bad_words\"] = bad_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 333 troll tweets that made mistakes.\n",
      "There are 228 non-troll tweets that made mistakes.\n",
      "That means that troll tweets comprise of 0.5935828877005348 of all mistakes.\n"
     ]
    }
   ],
   "source": [
    "oops = df.loc[df.grammar_errors > 0]\n",
    "print(\"There are \" + str(len(oops.loc[oops.trolliness == True])) + \" troll tweets that made mistakes.\")\n",
    "print(\"There are \" + str(len(oops.loc[oops.trolliness == False])) + \" non-troll tweets that made mistakes.\")\n",
    "print(\"That means that troll tweets comprise of {} of all mistakes.\".format(str(len(oops.loc[oops.trolliness == True]) / (len(oops.loc[oops.trolliness == True]) + len(oops.loc[oops.trolliness == False])))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>grammar_errors</th>\n",
       "      <th>bad_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200007</th>\n",
       "      <td>RT @Counselor70: @eff_dblu_ell @colavs2184 CNN &amp;amp; MSM TELL TRUTH OR GET THE FUCK OUT, THE\\r\\nSTENCH OF LIARS Those Bias Bastards https://t.co/d‚Ä¶</td>\n",
       "      <td>1</td>\n",
       "      <td>[truth]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200022</th>\n",
       "      <td>RT @tsbarnes89: ‚ÄòYou are fascinated with sex‚Äô: That Megyn Kelly-Newt Gingrich showdown was one for the ages https://t.co/7aRNLLARc7</td>\n",
       "      <td>1</td>\n",
       "      <td>[showdown]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200024</th>\n",
       "      <td>RT @JohnKStahlUSA: Share this with younger people in your life. We don't need another national embarrassment. #tcot #ccot #gop #maga https:‚Ä¶</td>\n",
       "      <td>1</td>\n",
       "      <td>[embarrassment]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200035</th>\n",
       "      <td>@DineshDSouza on Clintons: 'How do you go from zero to $300 million on a gov't. salary?'\\r\\nHmmü§î indeed?! https://t.co/5xmQxzGdbH</td>\n",
       "      <td>1</td>\n",
       "      <td>[clintons]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200041</th>\n",
       "      <td>Cops Refuse To Charge Teacher After He Body Slams 13-Year-Old Student So Hard His Leg Must Be Amputated‚Ä¶ https://t.co/TuEtODdoew</td>\n",
       "      <td>1</td>\n",
       "      <td>[body]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                       text  \\\n",
       "200007  RT @Counselor70: @eff_dblu_ell @colavs2184 CNN &amp; MSM TELL TRUTH OR GET THE FUCK OUT, THE\\r\\nSTENCH OF LIARS Those Bias Bastards https://t.co/d‚Ä¶   \n",
       "200022                  RT @tsbarnes89: ‚ÄòYou are fascinated with sex‚Äô: That Megyn Kelly-Newt Gingrich showdown was one for the ages https://t.co/7aRNLLARc7   \n",
       "200024         RT @JohnKStahlUSA: Share this with younger people in your life. We don't need another national embarrassment. #tcot #ccot #gop #maga https:‚Ä¶   \n",
       "200035                    @DineshDSouza on Clintons: 'How do you go from zero to $300 million on a gov't. salary?'\\r\\nHmmü§î indeed?! https://t.co/5xmQxzGdbH   \n",
       "200041                     Cops Refuse To Charge Teacher After He Body Slams 13-Year-Old Student So Hard His Leg Must Be Amputated‚Ä¶ https://t.co/TuEtODdoew   \n",
       "\n",
       "        grammar_errors        bad_words  \n",
       "200007               1          [truth]  \n",
       "200022               1       [showdown]  \n",
       "200024               1  [embarrassment]  \n",
       "200035               1       [clintons]  \n",
       "200041               1           [body]  "
      ]
     },
     "execution_count": 1120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('max_colwidth',200)\n",
    "oops[[\"text\", \"grammar_errors\",\"bad_words\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some examples of bad grammar in troll tweets being caught:\n",
    "  * \"I hope Julian Assange will deliver something that will make even **dumbest Americans** open their eyes.\"\n",
    "  * \"No one who thinks that Islam is peaceful should be a president! And we know **Hillary Clinton has muslim agenda**\"\n",
    "  * \"RT @Counselor70: @eff_dblu_ell @colavs2184 CNN &amp; **MSM TELL TRUTH** OR GET THE FUCK OUT, THE STENCH OF LIARS Those Bias Bastards https://t.co/d‚Ä¶\"\n",
    "  \n",
    " <br/> Obviously, it's not perfect. But it may be helpful as a feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Amount of Tags as Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This feature differs slightly from the rest of the features, as it does't deal with grammar so much as social presence. Again, studies on known troll tweets have shown that as part of their campaigns, trolls will tag other users at a higher-than-average rate to extend their influence and stir up the most discontent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1121,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = []\n",
    "for tweet in df.text:\n",
    "    toks = tokenizer.tokenize(tweet)\n",
    "    tweet_tags = 0\n",
    "    for tok in toks:\n",
    "        if tok[0] == \"@\":\n",
    "            tweet_tags += 1\n",
    "    tags.append(tweet_tags)\n",
    "df[\"tags\"] = tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it All Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I insert all the grammar features we have exctracted into the tfidf feature matrix using scipy's hstack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes as input a df column and returns a feature matrix\n",
    "def grammar_matrix(X):\n",
    "    import scipy\n",
    "    from scipy.sparse import coo_matrix, hstack\n",
    "    import numpy as np\n",
    "    mistake_matrix = []\n",
    "    for place, value in enumerate(X):\n",
    "        mini_matrix = [0.0] * 1\n",
    "        mini_matrix[0] = value\n",
    "        mistake_matrix.append(mini_matrix)\n",
    "    mistake_matrix = scipy.sparse.csr_matrix(mistake_matrix)\n",
    "    return mistake_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1148,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(df, df.trolliness)\n",
    "output2 = pd.DataFrame(\n",
    "    {\n",
    "        \"Actual\": y_test\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1149,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(tokenizer = tokenize_tweets)\n",
    "vectorizer.fit(x_train.text)\n",
    "features_train = vectorizer.transform(x_train.text)\n",
    "features_test = vectorizer.transform(x_test.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1138,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistake_train = grammar_matrix(x_train[\"grammar_errors\"])\n",
    "mistake_test = grammar_matrix(x_test[\"grammar_errors\"])\n",
    "\n",
    "avg_train = grammar_matrix(x_train[\"stop_score\"])\n",
    "avg_test = grammar_matrix(x_test[\"stop_score\"])\n",
    "\n",
    "tag_train = grammar_matrix(x_train[\"tags\"])\n",
    "tag_test = grammar_matrix(x_test[\"tags\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Hstack to Combine Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1150,
   "metadata": {},
   "outputs": [],
   "source": [
    "combof_train = hstack([mistake_train, features_train]).toarray()\n",
    "combof_test = hstack([mistake_test, features_test]).toarray()\n",
    "\n",
    "combof_train = hstack([avg_train, combof_train]).toarray()\n",
    "combof_test = hstack([avg_test, combof_test]).toarray()\n",
    "\n",
    "combof_train = hstack([tag_train, combof_train]).toarray()\n",
    "combof_test = hstack([tag_test, combof_test]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5218, 17384)\n",
      "(5218, 17387)\n"
     ]
    }
   ],
   "source": [
    "print(features_train.shape)\n",
    "print(combof_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see above that our 3 features have been succesfully added to the tfidf array (17387 - 17384 = **3**)\n",
    "\n",
    "<br/><br/> Instead of the MultinomialNB classifier previously used, I use the GaussianNB, since the MNB doesn't accept the negative feature values found in the \"stop_score\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just a quick test to see how number of estimators affects success score. The result is that it doesn't, really."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1054,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 80 estimators: \n",
      "\n",
      "Scores for model complex_forest\n",
      "Classification accuracy: 0.8839080459770114\n",
      "F1 score: 0.8839080459770114\n",
      "Precision: 0.8839080459770114\n",
      "Matthews coefficient: 0.7678695778363449\n",
      "\n",
      "With 85 estimators: \n",
      "\n",
      "Scores for model complex_forest\n",
      "Classification accuracy: 0.885632183908046\n",
      "F1 score: 0.885632183908046\n",
      "Precision: 0.885632183908046\n",
      "Matthews coefficient: 0.7712911468387591\n",
      "\n",
      "With 90 estimators: \n",
      "\n",
      "Scores for model complex_forest\n",
      "Classification accuracy: 0.889080459770115\n",
      "F1 score: 0.889080459770115\n",
      "Precision: 0.889080459770115\n",
      "Matthews coefficient: 0.7782049044385069\n",
      "\n",
      "With 95 estimators: \n",
      "\n",
      "Scores for model complex_forest\n",
      "Classification accuracy: 0.8885057471264368\n",
      "F1 score: 0.8885057471264368\n",
      "Precision: 0.8885057471264368\n",
      "Matthews coefficient: 0.777020590713656\n",
      "\n",
      "With 100 estimators: \n",
      "\n",
      "Scores for model complex_forest\n",
      "Classification accuracy: 0.8879310344827587\n",
      "F1 score: 0.8879310344827587\n",
      "Precision: 0.8879310344827587\n",
      "Matthews coefficient: 0.7758889603682616\n",
      "\n",
      "With 105 estimators: \n",
      "\n",
      "Scores for model complex_forest\n",
      "Classification accuracy: 0.8902298850574712\n",
      "F1 score: 0.8902298850574712\n",
      "Precision: 0.8902298850574712\n",
      "Matthews coefficient: 0.7804867738977641\n",
      "\n",
      "With 110 estimators: \n",
      "\n",
      "Scores for model complex_forest\n",
      "Classification accuracy: 0.8844827586206897\n",
      "F1 score: 0.8844827586206897\n",
      "Precision: 0.8844827586206897\n",
      "Matthews coefficient: 0.7690300399813197\n",
      "\n",
      "With 115 estimators: \n",
      "\n",
      "Scores for model complex_forest\n",
      "Classification accuracy: 0.889080459770115\n",
      "F1 score: 0.889080459770115\n",
      "Precision: 0.889080459770115\n",
      "Matthews coefficient: 0.7782260558535449\n",
      "\n",
      "With 120 estimators: \n",
      "\n",
      "Scores for model complex_forest\n",
      "Classification accuracy: 0.8879310344827587\n",
      "F1 score: 0.8879310344827587\n",
      "Precision: 0.8879310344827587\n",
      "Matthews coefficient: 0.7759059551488268\n",
      "\n",
      "With 125 estimators: \n",
      "\n",
      "Scores for model complex_forest\n",
      "Classification accuracy: 0.8833333333333333\n",
      "F1 score: 0.8833333333333333\n",
      "Precision: 0.8833333333333333\n",
      "Matthews coefficient: 0.7667310360132633\n",
      "\n",
      "With 130 estimators: \n",
      "\n",
      "Scores for model complex_forest\n",
      "Classification accuracy: 0.8862068965517241\n",
      "F1 score: 0.8862068965517241\n",
      "Precision: 0.8862068965517241\n",
      "Matthews coefficient: 0.7724485553016363\n",
      "\n",
      "With 135 estimators: \n",
      "\n",
      "Scores for model complex_forest\n",
      "Classification accuracy: 0.882183908045977\n",
      "F1 score: 0.882183908045977\n",
      "Precision: 0.882183908045977\n",
      "Matthews coefficient: 0.764432032045207\n",
      "\n",
      "With 140 estimators: \n",
      "\n",
      "Scores for model complex_forest\n",
      "Classification accuracy: 0.8850574712643678\n",
      "F1 score: 0.8850574712643678\n",
      "Precision: 0.8850574712643678\n",
      "Matthews coefficient: 0.77021861827591\n",
      "\n",
      "With 145 estimators: \n",
      "\n",
      "Scores for model complex_forest\n",
      "Classification accuracy: 0.8873563218390804\n",
      "F1 score: 0.8873563218390804\n",
      "Precision: 0.8873563218390804\n",
      "Matthews coefficient: 0.7747896208344076\n",
      "\n",
      "With 150 estimators: \n",
      "\n",
      "Scores for model complex_forest\n",
      "Classification accuracy: 0.8902298850574712\n",
      "F1 score: 0.8902298850574712\n",
      "Precision: 0.8902298850574712\n",
      "Matthews coefficient: 0.7804738196016223\n",
      "\n",
      "With 155 estimators: \n",
      "\n",
      "Scores for model complex_forest\n",
      "Classification accuracy: 0.8862068965517241\n",
      "F1 score: 0.8862068965517241\n",
      "Precision: 0.8862068965517241\n",
      "Matthews coefficient: 0.7724675280554214\n",
      "\n",
      "With 160 estimators: \n",
      "\n",
      "Scores for model complex_forest\n",
      "Classification accuracy: 0.889080459770115\n",
      "F1 score: 0.889080459770115\n",
      "Precision: 0.889080459770115\n",
      "Matthews coefficient: 0.7782260558535449\n",
      "\n",
      "With 165 estimators: \n",
      "\n",
      "Scores for model complex_forest\n",
      "Classification accuracy: 0.8862068965517241\n",
      "F1 score: 0.8862068965517241\n",
      "Precision: 0.8862068965517241\n",
      "Matthews coefficient: 0.7724675280554214\n",
      "\n",
      "With 170 estimators: \n",
      "\n",
      "Scores for model complex_forest\n",
      "Classification accuracy: 0.8867816091954023\n",
      "F1 score: 0.8867816091954022\n",
      "Precision: 0.8867816091954023\n",
      "Matthews coefficient: 0.7736280479174323\n",
      "\n",
      "With 175 estimators: \n",
      "\n",
      "Scores for model complex_forest\n",
      "Classification accuracy: 0.8867816091954023\n",
      "F1 score: 0.8867816091954022\n",
      "Precision: 0.8867816091954023\n",
      "Matthews coefficient: 0.7736280479174323\n",
      "\n",
      "With 180 estimators: \n",
      "\n",
      "Scores for model complex_forest\n",
      "Classification accuracy: 0.8867816091954023\n",
      "F1 score: 0.8867816091954022\n",
      "Precision: 0.8867816091954023\n",
      "Matthews coefficient: 0.7736070058591467\n",
      "\n",
      "With 185 estimators: \n",
      "\n",
      "Scores for model complex_forest\n",
      "Classification accuracy: 0.8873563218390804\n",
      "F1 score: 0.8873563218390804\n",
      "Precision: 0.8873563218390804\n",
      "Matthews coefficient: 0.7747665031649597\n",
      "\n",
      "With 190 estimators: \n",
      "\n",
      "Scores for model complex_forest\n",
      "Classification accuracy: 0.889080459770115\n",
      "F1 score: 0.889080459770115\n",
      "Precision: 0.889080459770115\n",
      "Matthews coefficient: 0.7782260558535449\n",
      "\n",
      "With 195 estimators: \n",
      "\n",
      "Scores for model complex_forest\n",
      "Classification accuracy: 0.8879310344827587\n",
      "F1 score: 0.8879310344827587\n",
      "Precision: 0.8879310344827587\n",
      "Matthews coefficient: 0.7759270518854886\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(80,200,5):\n",
    "    output2 = pd.DataFrame(\n",
    "    {\n",
    "        \"Actual\": y_test\n",
    "    })\n",
    "    clf = RandomForestClassifier(n_estimators=i)\n",
    "    clf.fit(combof_train, y_train)\n",
    "    output2[\"complex_forest\"] = clf.predict(combof_test)\n",
    "\n",
    "    print(\"With {} estimators: \".format(i))\n",
    "    print()\n",
    "    success2(\"complex_forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1153,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb_model = GaussianNB()\n",
    "gnb_model.fit(combof_train, y_train)\n",
    "output2[\"complex_GNB\"] = gnb_model.predict(combof_test)\n",
    "\n",
    "# Without grammar features\n",
    "# GNB doesn't accept sparse matrices; so the .toarray() is there\n",
    "gnb_model = GaussianNB()\n",
    "gnb_model.fit(features_train.toarray(), y_train)\n",
    "output2[\"default_GNB\"] = gnb_model.predict(features_test.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1154,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=150, max_depth = 115)\n",
    "clf.fit(combof_train, y_train)\n",
    "output2[\"complex_Forest\"] = clf.predict(combof_test)\n",
    "\n",
    "# Without grammar features\n",
    "clf = RandomForestClassifier(n_estimators=150, max_depth = 115)\n",
    "clf.fit(features_train, y_train)\n",
    "output2[\"default_Forest\"] = clf.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1155,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores for model default_MNB\n",
      "Classification accuracy: 0.8781609195402299\n",
      "F1 score: 0.8781609195402299\n",
      "Precision: 0.8781609195402299\n",
      "Matthews coefficient: 0.7567111872549714\n",
      "\n",
      "Scores for model default_Forest\n",
      "Classification accuracy: 0.8798850574712643\n",
      "F1 score: 0.8798850574712643\n",
      "Precision: 0.8798850574712643\n",
      "Matthews coefficient: 0.7600668919618172\n",
      "\n",
      "Scores for model complex_Forest\n",
      "Classification accuracy: 0.8913793103448275\n",
      "F1 score: 0.8913793103448275\n",
      "Precision: 0.8913793103448275\n",
      "Matthews coefficient: 0.7827638484750754\n",
      "\n",
      "Scores for model default_GNB\n",
      "Classification accuracy: 0.8270114942528736\n",
      "F1 score: 0.8270114942528736\n",
      "Precision: 0.8270114942528736\n",
      "Matthews coefficient: 0.6629135486987879\n",
      "\n",
      "Scores for model complex_GNB\n",
      "Classification accuracy: 0.8281609195402299\n",
      "F1 score: 0.8281609195402299\n",
      "Precision: 0.8281609195402299\n",
      "Matthews coefficient: 0.6647392291767612\n",
      "\n"
     ]
    }
   ],
   "source": [
    "success(\"default_MNB\")\n",
    "success2(\"default_Forest\", \"complex_Forest\", \"default_GNB\",\"complex_GNB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of this all, the increase in performance we get from adding in these extra features is relatively small. The Random Forest Classifier worked, in general, the best out of all the models. Adding in the grammar features improved the model's F1 score by .0115. Implementing each individual feature one at a time yielded similar, smaller increases in performance (I didn't include it in the notebook because it was very verbose and took a long time to run). \n",
    "\n",
    "<br><br> My first instinct when I saw the lackluster differences in success rates between the default and complex models was to find out how to alter the weights for each feature, i.e. tell the model to prioritize the stop_score of a specific tweet over the text of the tweet. Yet, the whole point of machine learning is to use statistical to assign optimal weights for each feature. For me to intervene in the weighting process would alter the entire \"machine learning\" portion of the project.\n",
    "\n",
    "<br><br> If I were to come back to this project later, I may experiment with the text I used to calibrate the grammar features. For instance, I may use a less formal corpus (like tweets) rather than a formal corpus, like the congressional hearing I used. One thing I read in one of the many articles I read on Russian troll tweets is that they tend to form echo chambers of tagging within their posse of trolls; a troll is likely to tweet at/converses with another troll. With this in mind, I may be able to construct some sort of feature where, if the confidence level of a tweet being a troll is above a threshold, the user's handle is on a \"blacklist\" and all future tweets to tag that user are given a higher \"suspicion\" score as a result of that tag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspirations/Sources for the Project:\n",
    "https://medium.com/@conspirator0/identifying-political-bot-troll-social-media-activity-using-machine-learning-20dcd56e961a \n",
    "<br>\n",
    "https://fivethirtyeight.com/features/what-you-found-in-3-million-russian-troll-tweets/\n",
    "<br>\n",
    "https://medium.com/dfrlab/trolltracker-how-to-spot-russian-trolls-2f6d3d287eaa\n",
    "<br>\n",
    "https://arxiv.org/pdf/1901.11162.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python[datascience",
   "language": "python",
   "name": "datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
